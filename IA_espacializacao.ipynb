{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3fea6f",
   "metadata": {},
   "source": [
    "# IA para espacialização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9361a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asciichartpy\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from pyproj import Transformer\n",
    "from utils.consts import SOIL_TYPES\n",
    "from rasterio.transform import rowcol\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d9be1",
   "metadata": {},
   "source": [
    "### Device disponível para treinar o modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a26024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1) Dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d56c5",
   "metadata": {},
   "source": [
    "### Dataset com os dados para convolução\n",
    "---\n",
    "- O Dataset nesta aplicação é extremamente importante pois irá englobar todas as informações necessárias para espacializar os dado de infiltração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo Tabelas\n",
      "Lendo Rasteres\n",
      "Len: 83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([9, 25, 25]), torch.Size([3, 9, 25, 25]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Dataset\n",
    "class MeuDataset(Dataset):\n",
    "    nix_bands = [\n",
    "        \"R400 nm\", \"R410 nm\", \"R420 nm\", \"R430 nm\",\n",
    "        \"R440 nm\", \"R450 nm\", \"R460 nm\", \"R470 nm\",\n",
    "        \"R480 nm\", \"R490 nm\", \"R500 nm\", \"R510 nm\",\n",
    "        \"R520 nm\", \"R530 nm\", \"R540 nm\", \"R550 nm\",\n",
    "        \"R560 nm\", \"R570 nm\", \"R580 nm\", \"R590 nm\",\n",
    "        \"R600 nm\", \"R610 nm\", \"R620 nm\", \"R630 nm\",\n",
    "        \"R640 nm\", \"R650 nm\", \"R660 nm\", \"R670 nm\",\n",
    "        \"R680 nm\", \"R690 nm\", \"R700 nm\",\n",
    "    ]\n",
    "\n",
    "    columns_dados = [\"Ponto\", \"Lat\", \"Lon\", \"soils_type\", \"Clay\", \"Silt\", \"Sand\", \"K (C1)\"]\n",
    "\n",
    "    def __init__(self, device:torch.device|None=None):\n",
    "        \"\"\"\n",
    "        O dataset tem o formato de uma tupla com os valores em X e em Y:\n",
    "        - (X, Y)\n",
    "        - Onde:\n",
    "        - X: (rasters_vals, [Dist. Talvegue, Tipo do Solo, na ordem de soil_types])\n",
    "        - Y: K (cm/s)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.janela = 25 # Janela deve ser ímpar\n",
    "\n",
    "        if self.janela%2 == 0:\n",
    "            raise ValueError(\"A janela deve ser ímpar\")\n",
    "\n",
    "        # Lendo Tabelas\n",
    "        print(\"Lendo Tabelas\")\n",
    "        self.dados = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"Infiltracao\")\n",
    "        self.dados = self.dados[self.columns_dados].dropna().reset_index(drop=True)\n",
    "        gdf = gpd.GeoDataFrame(self.dados, geometry=gpd.points_from_xy(self.dados[\"Lat\"], self.dados[\"Lon\"]), crs=\"EPSG:4326\")\n",
    "        gdf.to_crs(\"EPSG:31983\", inplace=True)\n",
    "        self.dados['Lat'] = gdf.geometry.y\n",
    "        self.dados['Lon'] = gdf.geometry.x\n",
    "\n",
    "        self.nix = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"Nix\")\n",
    "        self.pXRF = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"pXRF\")\n",
    "\n",
    "        # Lendo Rasteres importantes\n",
    "        self.talvegues = gpd.read_file(r\"D:/Mestrado/Trabalho Final/SIG/HidrografiaArea.zip\")\n",
    "\n",
    "        # Lendo Rasteres\n",
    "        print(\"Lendo Rasteres\")\n",
    "        self.uso_solo        = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/USOSOLO.tif\")                 # Tipos de uso do solo\n",
    "        self.elevation       = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Elevation.tif\")               # Elevação\n",
    "        self.terrain_rug_idx = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/TerrainRuggednessIndex.tif\")  # Variação de elevação entre um pixel e seus vizinhos imediatos\n",
    "        self.topo_pos_idx    = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/TopograficPositionIndex.tif\") # Elevação de um ponto com a média da elevação ao redor, topo, vale ou plano\n",
    "        self.roughness       = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Roughness.tif\")               # A diferença entre a elevação máxima e mínima dentro de uma vizinhança\n",
    "        self.slope           = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Slope.tif\")                   # Declividade\n",
    "        self.aspect          = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Aspect.tif\")                  # Para onde \"aponta\" a face do terreno\n",
    "        self.texture_02      = rxr.open_rasterio(r\"D:\\Mestrado\\Trabalho Final\\SIG\\textura_2.tif\")               # Textura a 2 cm\n",
    "        self.texture_20      = rxr.open_rasterio(r\"D:\\Mestrado\\Trabalho Final\\SIG\\textura_20.tif\")              # Textura a 20 cm\n",
    "\n",
    "        # Dados dos rasteres concatenados\n",
    "        self.raster_data = np.array([\n",
    "            self.uso_solo.values[0],\n",
    "            self.elevation.values[0],\n",
    "            self.terrain_rug_idx.values[0],\n",
    "            self.topo_pos_idx.values[0],\n",
    "            self.roughness.values[0],\n",
    "            self.slope.values[0],\n",
    "            self.aspect.values[0],\n",
    "            self.texture_02.values[0],\n",
    "            self.texture_20.values[0],\n",
    "        ])\n",
    "\n",
    "        self.transformer = Transformer.from_crs(\"EPSG:31983\", self.uso_solo.rio.crs, always_xy=True)\n",
    "        self.transform = self.uso_solo.rio.transform()\n",
    "\n",
    "        self._process_dados()\n",
    "\n",
    "    def _process_dados(self):\n",
    "        # Dados para convolução\n",
    "        x, y = self.transformer.transform(self.dados[\"Lon\"], self.dados[\"Lat\"])\n",
    "        row, col = rowcol(self.transform, x, y)\n",
    "\n",
    "        jan = int((self.janela-1)/2)\n",
    "\n",
    "        start_row = row-jan\n",
    "        end_row   = row+jan\n",
    "\n",
    "        start_col = col-jan\n",
    "        end_col   = col+jan\n",
    "\n",
    "        self.dados['s_row']=start_row\n",
    "        self.dados['e_row']=end_row\n",
    "        self.dados['s_col']=start_col\n",
    "        self.dados['e_col']=end_col\n",
    "\n",
    "        # Distância até o talvegue principal\n",
    "        linhas_unidas = self.talvegues.union_all()\n",
    "        pontos = gpd.GeoSeries(gpd.points_from_xy(self.dados[\"Lon\"], self.dados[\"Lat\"]), crs=\"EPSG:31983\")\n",
    "        dists = pontos.apply(lambda p: p.distance(linhas_unidas))\n",
    "        self.dados[\"dist_talvegue\"] = dists\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dados)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = i\n",
    "        if isinstance(i, (int, float)):\n",
    "            idx = [i]\n",
    "\n",
    "        # Pontos\n",
    "        pontos = self.dados.loc[idx]\n",
    "\n",
    "        # Valores nos rasteres\n",
    "        rasters_vals = []\n",
    "        for s_row, e_row, s_col, e_col in zip(pontos['s_row'], pontos['e_row'], pontos['s_col'], pontos['e_col']):\n",
    "            rasters_vals.append(self.raster_data[:, s_row:e_row+1, s_col:e_col+1])\n",
    "        \n",
    "        rasters_vals = torch.tensor(np.array(rasters_vals), device=self.device, dtype=torch.float64)\n",
    "\n",
    "        # Demais dados\n",
    "        K = torch.tensor(pontos[\"K (C1)\"].values, device=self.device) * 1000 # K*1000 pois os valores estão baixos de mais\n",
    "        dist_talvegue = torch.tensor(pontos[[\"dist_talvegue\"]].values, device=self.device)\n",
    "\n",
    "        if isinstance(i, (int, float)):\n",
    "            dist_talvegue = dist_talvegue[0]\n",
    "            rasters_vals = rasters_vals[0]\n",
    "            K = K[0]\n",
    "\n",
    "\n",
    "        return (rasters_vals, dist_talvegue), K\n",
    "    \n",
    "dataset = MeuDataset(device=device)\n",
    "\n",
    "print(\"Len:\", len(dataset))\n",
    "dataset[15][0][0].shape, dataset[1:3][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1aafd",
   "metadata": {},
   "source": [
    "### Configurações do treino\n",
    "---\n",
    "\n",
    "- Seed para números aleatórios\n",
    "- % de treino e teste\n",
    "- Métricas\n",
    "- Epochs\n",
    "- Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed para permitir reprodutibilidade dos valores pseudo-aleatórios\n",
    "seed = 42\n",
    "\n",
    "# Porcentagens de Treino e Teste\n",
    "train_percent = 80\n",
    "test_percent  = 20\n",
    "\n",
    "# BatchSize e Epochs\n",
    "batch_size = 2\n",
    "epochs     = 1000  # Poucos pontos, verificar overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9baf77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Total: 83 N Train: 70 N Teste: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([4, 9, 25, 25]), torch.Size([4, 9, 25, 25]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(dataset)\n",
    "n_train = int(train_percent*n/100)\n",
    "n_test = n - n_train\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "train_ds, test_ds = random_split(dataset, [n_train, n_test], generator=g)\n",
    "\n",
    "print(\"N Total:\", n, \"N Train:\", n_train, \"N Teste:\", n_test)\n",
    "\n",
    "train_ds[1:5][0][0].shape, test_ds[1:5][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0f5b7",
   "metadata": {},
   "source": [
    "### MLP e CNN\n",
    "---\n",
    "\n",
    "MLP configurada com uma CNN também"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b64aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1069],\n",
       "        [0.1090],\n",
       "        [0.1053],\n",
       "        [0.1080]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo (MLP e CNN)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=9,    # Número de bandas do raster de entrada\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        ).to(dtype=torch.float64)\n",
    "        self.flatten = nn.Flatten().to(dtype=torch.float64)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                (1152) + 1, # Tamanho da saída da convolução + dist_talvegue\n",
    "                64\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16, 1)\n",
    "        ).to(dtype=torch.float64)\n",
    "\n",
    "    def forward(self, raster, values):\n",
    "        x = self.conv(raster)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.cat([x, values], dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP().to(device=device)\n",
    "\n",
    "X = dataset[1:4][0]\n",
    "mlp(*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9fcac",
   "metadata": {},
   "source": [
    "### Ajustar o modelo MLP\n",
    "\n",
    "- Processos para ajustar a MLP pelo método do gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5f89fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nse(y_pred, y_true, mean=None):\n",
    "    if mean is not None:\n",
    "        y_true_mean = mean\n",
    "    else:\n",
    "        y_true_mean = torch.mean(y_true)\n",
    "        \n",
    "    numerator = torch.sum((y_pred - y_true) ** 2)\n",
    "    denominator = torch.sum((y_true - y_true_mean) ** 2)\n",
    "    return 1 - (numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f431267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000 | Train Loss: 95.1472 | Val Loss: 152.2005 | Kc NASH: 0.093644cm/s | At lr:1.0990e-03\n",
      "  231.77  ┼╮\n",
      "  226.27  ┤│\n",
      "  220.77  ┤│\n",
      "  215.27  ┤│\n",
      "  209.78  ┤│\n",
      "  204.28  ┤│\n",
      "  198.78  ┤│\n",
      "  193.28  ┤│\n",
      "  187.79  ┤│\n",
      "  182.29  ┤│\n",
      "  176.79  ┤│\n",
      "  171.29  ┤╰╮                                        ╭╮                                ╭╮\n",
      "  165.80  ┤ │         ╭╮  ╭╮                       ╭╮││     ╭╮                     ╭╮  ││                                                     ╭╮\n",
      "  160.30  ┤ │       ╭╮││  ││ ╭─╮    ╭╮             │╰╯╰╮╭╮ ╭╯│              ╭╮     ││  ││                 ╭╮ ╭╮ ╭─╮ ╭─╮╭╮                ╭╮   ││          ╭╮\n",
      "  154.80  ┤ ╰─────╮╭╯╰╯╰╮╭╯╰─╯ ╰────╯╰─────────╮╭──╯   │││ │ │╭─────────────╯╰──╮╭─╯│  │╰╮╭───────────────╯╰─╯╰─╯ ╰╮│ ╰╯╰────────────────╯╰──╮│╰─╮╭╮╭╮╭───╯╰────\n",
      "  149.30  ┤       ╰╯    ╰╯                     ╰╯      ╰╯╰─╯ ╰╯                 ╰╯  ╰──╯ ╰╯                        ╰╯                        ╰╯  ╰╯╰╯╰╯\n",
      "  143.81  ┤\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"best_model/\", exist_ok=True)\n",
    "\n",
    "# Média dos valores para compara erros\n",
    "mean = dataset[:][1].mean()\n",
    "\n",
    "model = MLP().to(device=device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=0.0001,      # menor LR\n",
    "    max_lr=0.1,          # maior LR\n",
    "    step_size_up=100,    # número de iterações até atingir o max_lr\n",
    "    mode='triangular',   # ou 'triangular2', 'exp_range'\n",
    "    cycle_momentum=True  # necessário se usar Adam em vez de SGD\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "best_test_loss = torch.inf\n",
    "best_state = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # --- treino ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_sim = model(*x)\n",
    "\n",
    "        loss = criterion(y_sim[:, 0], y)\n",
    "        running_loss += loss.item() * x[1].size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset) # type: ignore\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # --- validação ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    nash = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            # Forward\n",
    "            y_sim = model(*x)\n",
    "\n",
    "            loss = criterion(y_sim[:, 0], y)\n",
    "            val_loss += loss.item() * x[1].size(0)\n",
    "\n",
    "            ns = nse(y_sim[:, 0], y, mean)\n",
    "            nash += ns.item() * x[1].size(0)\n",
    "\n",
    "    # Média do loss\n",
    "    val_loss /= len(test_loader.dataset) # type: ignore\n",
    "    nash /= len(test_loader.dataset)     # type: ignore\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # early stopping simples\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if val_loss < best_test_loss:\n",
    "        best_test_loss = val_loss\n",
    "        best_state = {\n",
    "            \"lr\":current_lr,\n",
    "            \"epoch\": epoch,\n",
    "            \"epochs\":epochs,\n",
    "            \"val_loss\":val_loss,\n",
    "            \"train_loss\":train_loss,\n",
    "            \"batch_size\":batch_size,\n",
    "            \"model_state\":model.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(best_state, f\"best_model/{str(val_loss).replace(\".\", \"_\")}.pth\")\n",
    "\n",
    "    # Altero a taxa de aprendizado a cada fim da epoch para melhorar o aprendizado\n",
    "    scheduler.step()\n",
    "\n",
    "    #Print das métricas atuais\n",
    "\n",
    "    indices = np.linspace(0, len(val_losses) - 1, 150, dtype=int)\n",
    "    subset = [val_losses[i] for i in indices]\n",
    "\n",
    "    ascii_chart = asciichartpy.plot(subset, {'height': 15})\n",
    "\n",
    "    # Limpo o terminal\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Kc NASH: {(nash):.6f}cm/s | At lr:{current_lr:.4e}\\n{ascii_chart}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
