{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3fea6f",
   "metadata": {},
   "source": [
    "# IA para espacialização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9361a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import asciichartpy\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from pyproj import Transformer\n",
    "from shapely import Point, distance\n",
    "from rasterio.transform import rowcol\n",
    "from IPython.display import clear_output\n",
    "from utils.consts import SOIL_TYPES, USO_SOLO_CLASS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CyclicLR\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d9be1",
   "metadata": {},
   "source": [
    "### Device disponível para treinar o modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5a26024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1) Dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d56c5",
   "metadata": {},
   "source": [
    "### Dataset com os dados para convolução\n",
    "---\n",
    "- O Dataset nesta aplicação é extremamente importante pois irá englobar todas as informações necessárias para espacializar os dado de infiltração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo Tabelas\n",
      "Lendo Rasteres\n",
      "Processando dados\n",
      "Len: 83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([12, 15, 15]), torch.Size([10, 12, 15, 15]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MeuDataset(Dataset):\n",
    "    nix_bands = [\n",
    "        \"R400 nm\", \"R410 nm\", \"R420 nm\", \"R430 nm\",\n",
    "        \"R440 nm\", \"R450 nm\", \"R460 nm\", \"R470 nm\",\n",
    "        \"R480 nm\", \"R490 nm\", \"R500 nm\", \"R510 nm\",\n",
    "        \"R520 nm\", \"R530 nm\", \"R540 nm\", \"R550 nm\",\n",
    "        \"R560 nm\", \"R570 nm\", \"R580 nm\", \"R590 nm\",\n",
    "        \"R600 nm\", \"R610 nm\", \"R620 nm\", \"R630 nm\",\n",
    "        \"R640 nm\", \"R650 nm\", \"R660 nm\", \"R670 nm\",\n",
    "        \"R680 nm\", \"R690 nm\", \"R700 nm\",\n",
    "    ]\n",
    "\n",
    "    columns_dados = [\"Ponto\", \"Lat\", \"Lon\", \"soils_type\", \"Clay\", \"Silt\", \"Sand\", \"K (C1)\"]\n",
    "\n",
    "    columns_uso_solo = [1, 2, 5, 6, 8, 9]\n",
    "\n",
    "    def __init__(self, device:torch.device|None=None, eval=False):\n",
    "        \"\"\"\n",
    "        O dataset tem o formato de uma tupla com os valores em X e em Y:\n",
    "        - (X, Y)\n",
    "        - Onde:\n",
    "        - X: (rasters_vals, [Dist. Talvegue, Tipo do Solo, na ordem de soil_types])\n",
    "        - Y: K (cm/s)\n",
    "        \"\"\"\n",
    "        self.eval = eval\n",
    "        self.device = device\n",
    "        self.janela = 15 # Janela deve ser ímpar\n",
    "\n",
    "        if self.janela%2 == 0:\n",
    "            raise ValueError(\"A janela deve ser ímpar\")\n",
    "\n",
    "        # Lendo Tabelas\n",
    "        print(\"Lendo Tabelas\")\n",
    "        self.dados = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"Infiltracao\")\n",
    "        self.dados = self.dados[self.columns_dados].dropna().reset_index(drop=True)\n",
    "        gdf = gpd.GeoDataFrame(self.dados, geometry=gpd.points_from_xy(self.dados[\"Lat\"], self.dados[\"Lon\"]), crs=\"EPSG:4326\")\n",
    "        gdf.to_crs(\"EPSG:31983\", inplace=True)\n",
    "        self.dados['Lat'] = gdf.geometry.y\n",
    "        self.dados['Lon'] = gdf.geometry.x\n",
    "\n",
    "        self.nix = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"Nix\")\n",
    "        self.pXRF = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"pXRF\")\n",
    "\n",
    "        # Lendo Rasteres importantes\n",
    "        self.talvegues = gpd.read_file(r\"D:/Mestrado/Trabalho Final/SIG/HidrografiaArea.zip\")\n",
    "\n",
    "        # Lendo Rasteres\n",
    "        print(\"Lendo Rasteres\")\n",
    "        self.uso_solo        = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/USOSOLO.tif\")                 # Tipos de uso do solo\n",
    "        self.elevation       = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Elevation.tif\")               # Elevação\n",
    "        self.terrain_rug_idx = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/TerrainRuggednessIndex.tif\")  # Variação de elevação entre um pixel e seus vizinhos imediatos\n",
    "        self.topo_pos_idx    = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/TopograficPositionIndex.tif\") # Elevação de um ponto com a média da elevação ao redor, topo, vale ou plano\n",
    "        self.roughness       = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Roughness.tif\")               # A diferença entre a elevação máxima e mínima dentro de uma vizinhança\n",
    "        self.slope           = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Slope.tif\")                   # Declividade\n",
    "        self.aspect          = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Aspect.tif\")                  # Para onde \"aponta\" a face do terreno\n",
    "        self.texture_02      = rxr.open_rasterio(r\"D:\\Mestrado\\Trabalho Final\\SIG\\textura_2.tif\")               # Textura a 2 cm\n",
    "        self.texture_20      = rxr.open_rasterio(r\"D:\\Mestrado\\Trabalho Final\\SIG\\textura_20.tif\")              # Textura a 20 cm\n",
    "        \n",
    "        # X e Y dos valores\n",
    "        if self.eval:\n",
    "            xx, yy = np.meshgrid(self.uso_solo.x.values, self.uso_solo.y.values) # type: ignore\n",
    "            self.x_y = np.column_stack([xx.ravel(), yy.ravel()])                 # type: ignore\n",
    "\n",
    "        self.transformer = Transformer.from_crs(\"EPSG:31983\", self.uso_solo.rio.crs, always_xy=True) # type: ignore\n",
    "        self.transform = self.uso_solo.rio.transform() # type: ignore\n",
    "\n",
    "        print(\"Processando dados\")\n",
    "        self._process_dados()\n",
    "\n",
    "    def _process_dados(self):\n",
    "        # Dados dos rasteres concatenados\n",
    "        self.raster_data = np.array([\n",
    "            self._norm_data(self.elevation.values[0]), # type: ignore\n",
    "            self._norm_data(self.terrain_rug_idx.values[0]), # type: ignore\n",
    "            self._norm_data(self.topo_pos_idx.values[0]), # type: ignore\n",
    "            self._norm_data(self.roughness.values[0]), # type: ignore\n",
    "            self._norm_data(self.slope.values[0]), # type: ignore\n",
    "            self._norm_data(self.aspect.values[0]), # type: ignore\n",
    "            self.uso_solo.values[0], # type: ignore\n",
    "            self.texture_20.values[0], # type: ignore\n",
    "        ])\n",
    "\n",
    "        # Dados para convolução\n",
    "        x, y = self.transformer.transform(self.dados[\"Lon\"], self.dados[\"Lat\"])\n",
    "        row, col = rowcol(self.transform, x, y)\n",
    "\n",
    "        jan = int((self.janela-1)/2)\n",
    "\n",
    "        start_row = row-jan\n",
    "        end_row   = row+jan\n",
    "\n",
    "        start_col = col-jan\n",
    "        end_col   = col+jan\n",
    "\n",
    "        self.dados['s_row']=start_row\n",
    "        self.dados['e_row']=end_row\n",
    "        self.dados['s_col']=start_col\n",
    "        self.dados['e_col']=end_col\n",
    "\n",
    "        # Distância até o talvegue principal\n",
    "        self.linhas_unidas = self.talvegues.union_all()\n",
    "        pontos = gpd.GeoSeries(gpd.points_from_xy(self.dados[\"Lon\"], self.dados[\"Lat\"]), crs=\"EPSG:31983\")\n",
    "        dists = pontos.apply(lambda p: p.distance(self.linhas_unidas))\n",
    "        self.dados[\"dist_talvegue\"] = dists\n",
    "\n",
    "    def _norm_data(self, data:np.ndarray)->np.ndarray:\n",
    "        return (data - data.min())/(data.max()-data.min())\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.uso_solo.size if self.eval else len(self.dados) # type: ignore\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = i\n",
    "        if isinstance(i, (int, float)):\n",
    "            idx = [i]\n",
    "        elif self.eval and (isinstance(i, slice) or isinstance(i, (list, np.ndarray))):\n",
    "            if isinstance(i, slice):\n",
    "                indices = list(range(*i.indices(len(self))))\n",
    "            else:\n",
    "                indices = i\n",
    "            \n",
    "            rasters = []\n",
    "            values = []\n",
    "            for idx in indices:\n",
    "                raster, value = self.__getitem__(idx)\n",
    "                rasters.append(raster[0])\n",
    "                values.append(value[0])\n",
    "\n",
    "            rasters = torch.stack(rasters)\n",
    "            values = torch.stack(values)\n",
    "            return rasters, values\n",
    "\n",
    "        # Modo de gerar os dados finais\n",
    "        if self.eval:\n",
    "            x_y = self.x_y[i] # type: ignore\n",
    "            x, y = self.transformer.transform(x_y[0], x_y[1])\n",
    "            row, col = rowcol(self.transform, x, y)\n",
    "\n",
    "            bands, height, width = self.raster_data.shape\n",
    "\n",
    "            jan = int((self.janela-1)/2)\n",
    "            window_shape = (bands, 2*jan + 1, 2*jan + 1)\n",
    "\n",
    "            # Criar janela preenchida com NaN\n",
    "            janela_data = np.full(window_shape, np.nan, dtype=self.raster_data.dtype)\n",
    "\n",
    "            # Calcular limites válidos dentro do raster\n",
    "            row_min = max(0, row - jan) # type: ignore\n",
    "            row_max = min(height, row + jan + 1) # type: ignore\n",
    "\n",
    "            col_min = max(0, col - jan) # type: ignore\n",
    "            col_max = min(width, col + jan + 1) # type: ignore\n",
    "            \n",
    "            # Limites relativos à janela\n",
    "            win_row_start = row_min - (row - jan)\n",
    "            win_row_end   = win_row_start + (row_max - row_min)\n",
    "\n",
    "            win_col_start = col_min - (col - jan)\n",
    "            win_col_end   = win_col_start + (col_max - col_min)\n",
    "            \n",
    "            # Copiar a parte válida do raster para a janela\n",
    "            janela_data[:, win_row_start:win_row_end, win_col_start:win_col_end] = self.raster_data[:, row_min:row_max, col_min:col_max]\n",
    "\n",
    "            # Distância do talvegue\n",
    "            p = Point(x, y)\n",
    "            dist = torch.tensor(np.array([[distance(p, self.linhas_unidas)]]), device=self.device)\n",
    "\n",
    "            rasters_vals = torch.tensor(np.array([janela_data]), device=self.device, dtype=torch.float64)\n",
    "\n",
    "            return (rasters_vals, dist)\n",
    "\n",
    "        # Pontos\n",
    "        pontos = self.dados.loc[idx]\n",
    "\n",
    "        # Valores nos rasteres\n",
    "        rasters_vals = []\n",
    "        for s_row, e_row, s_col, e_col in zip(pontos['s_row'], pontos['e_row'], pontos['s_col'], pontos['e_col']):\n",
    "            rasters = np.array([])\n",
    "\n",
    "            usos = []\n",
    "            for uso_solo in self.columns_uso_solo:\n",
    "                uso = np.where(self.raster_data[6, s_row:e_row+1, s_col:e_col+1]==uso_solo, 1, 0)\n",
    "                usos.append(uso)\n",
    "            usos = np.array(usos)\n",
    "\n",
    "            tipos = []\n",
    "            for idx, tipo_solo in enumerate(SOIL_TYPES):\n",
    "                if tipo_solo is None:\n",
    "                    continue\n",
    "                tipo = np.where(self.raster_data[7, s_row:e_row+1, s_col:e_col+1]==idx, 1, 0)\n",
    "                tipos.append(tipo)\n",
    "            tipos = np.array(tipos)\n",
    "\n",
    "            outras_bandas = self.raster_data[:6, s_row:e_row+1, s_col:e_col+1]\n",
    "            raster_stack = np.concatenate([usos, tipos, outras_bandas], axis=0)\n",
    "            rasters_vals.append(raster_stack)\n",
    "        \n",
    "        rasters_vals = torch.tensor(np.array(rasters_vals), device=self.device, dtype=torch.float64)\n",
    "\n",
    "        # Demais dados\n",
    "        K = torch.tensor(pontos[\"K (C1)\"].values, device=self.device) * 1000 # K*1000 pois os valores estão baixos de mais\n",
    "        dist_talvegue = torch.tensor(pontos[[\"dist_talvegue\"]].values, device=self.device)\n",
    "\n",
    "        if isinstance(i, (int, float)):\n",
    "            dist_talvegue = dist_talvegue[0]\n",
    "            rasters_vals = rasters_vals[0]\n",
    "            K = K[0]\n",
    "\n",
    "\n",
    "        return (rasters_vals, dist_talvegue), K\n",
    "    \n",
    "dataset = MeuDataset(device=device)\n",
    "\n",
    "print(\"Len:\", len(dataset))\n",
    "dataset[15][0][0].shape, dataset[1:10][0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1aafd",
   "metadata": {},
   "source": [
    "### Configurações do treino\n",
    "---\n",
    "\n",
    "- Seed para números aleatórios\n",
    "- % de treino e teste\n",
    "- Métricas\n",
    "- Epochs\n",
    "- Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed para permitir reprodutibilidade dos valores pseudo-aleatórios\n",
    "seed = 42\n",
    "\n",
    "# Porcentagens de Treino e Teste\n",
    "train_percent = 85\n",
    "test_percent  = 15\n",
    "\n",
    "# BatchSize e Epochs\n",
    "batch_size = 2\n",
    "epochs     = 1000  # Poucos pontos, verificar overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9baf77e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(dataset)\n",
    "n_train = int(train_percent*n/100)\n",
    "n_test = n - n_train\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "train_ds, test_ds = random_split(dataset, [n_train, n_test], generator=g)\n",
    "\n",
    "print(\"N Total:\", n, \"N Train:\", n_train, \"N Teste:\", n_test)\n",
    "\n",
    "train_ds[1:5][0][0].shape, test_ds[1:5][0][0].shape # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0f5b7",
   "metadata": {},
   "source": [
    "### MLP e CNN\n",
    "---\n",
    "\n",
    "MLP configurada com uma CNN também"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b64aa68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo (MLP e CNN)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(34, 64),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 1),\n",
    "        ).to(dtype=torch.float64)\n",
    "\n",
    "    def forward(self, values):\n",
    "        # x = self.conv(raster)\n",
    "        # x = self.flatten(x)\n",
    "        # x = torch.cat([x, values], dim=1)\n",
    "        x = self.fc(values)\n",
    "        return x\n",
    "\n",
    "mlp = MLP().to(device=device)\n",
    "\n",
    "X = dataset[1:4][0]\n",
    "mlp(*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9fcac",
   "metadata": {},
   "source": [
    "### Ajustar o modelo MLP\n",
    "\n",
    "- Processos para ajustar a MLP pelo método do gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f89fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nse(y_pred, y_true, mean=None):\n",
    "    if mean is not None:\n",
    "        y_true_mean = mean\n",
    "    else:\n",
    "        y_true_mean = torch.mean(y_true)\n",
    "        \n",
    "    numerator = torch.sum((y_pred - y_true) ** 2)\n",
    "    denominator = torch.sum((y_true - y_true_mean) ** 2)\n",
    "    return 1 - (numerator / denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f431267",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"best_model/\", exist_ok=True)\n",
    "\n",
    "# Média dos valores para compara erros\n",
    "mean = dataset[:][1].mean()\n",
    "\n",
    "model = MLP().to(device=device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = CyclicLR(\n",
    "    optimizer,\n",
    "    base_lr=0.0001,      # menor LR\n",
    "    max_lr=0.1,          # maior LR\n",
    "    step_size_up=100,    # número de iterações até atingir o max_lr\n",
    "    mode='triangular',   # ou 'triangular2', 'exp_range'\n",
    "    cycle_momentum=True  # necessário se usar Adam em vez de SGD\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "best_test_loss = torch.inf\n",
    "best_state = None\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # --- treino ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_sim = model(*x)\n",
    "\n",
    "        loss = criterion(y_sim[:, 0], y)\n",
    "        running_loss += loss.item() * x[0].size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset) # type: ignore\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # --- validação ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    nash = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            # Forward\n",
    "            y_sim = model(*x)\n",
    "\n",
    "            loss = criterion(y_sim[:, 0], y)\n",
    "            val_loss += loss.item() * x[0].size(0)\n",
    "\n",
    "            ns = nse(y_sim[:, 0], y, mean)\n",
    "            nash += ns.item() * x[0].size(0)\n",
    "\n",
    "    # Média do loss\n",
    "    val_loss /= len(test_loader.dataset) # type: ignore\n",
    "    nash /= len(test_loader.dataset)     # type: ignore\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # early stopping simples\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    if val_loss < best_test_loss and epoch > 100:\n",
    "        best_test_loss = val_loss\n",
    "        best_state = {\n",
    "            \"lr\":current_lr,\n",
    "            \"epoch\": epoch,\n",
    "            \"epochs\":epochs,\n",
    "            \"val_loss\":val_loss,\n",
    "            \"train_loss\":train_loss,\n",
    "            \"batch_size\":batch_size,\n",
    "            \"model_state\":model.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(best_state, f\"best_model/{str(val_loss).replace(\".\", \"_\")}.pth\")\n",
    "\n",
    "    # Altero a taxa de aprendizado a cada fim da epoch para melhorar o aprendizado\n",
    "    scheduler.step()\n",
    "\n",
    "    #Print das métricas atuais\n",
    "\n",
    "    indices = np.linspace(0, len(val_losses) - 1, 150, dtype=int)\n",
    "    subset = [val_losses[i] for i in indices]\n",
    "\n",
    "    ascii_chart = asciichartpy.plot(subset, {'height': 15})\n",
    "\n",
    "    # Limpo o terminal\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Kc NASH: {(nash):.6f}cm/s | At lr:{current_lr:.4e}\\n{ascii_chart}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
