{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a3fea6f",
   "metadata": {},
   "source": [
    "# IA para espacialização dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9361a643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rasterio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import rioxarray as rxr\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from pyproj import Transformer\n",
    "from rasterio.transform import rowcol\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, random_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d9be1",
   "metadata": {},
   "source": [
    "### Device disponível para treinar o modelo MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5a26024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1) Dispositivo (GPU se disponível)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d56c5",
   "metadata": {},
   "source": [
    "### Dataset com os dados para convolução\n",
    "---\n",
    "- O Dataset nesta aplicação é extremamente importante pois irá englobar todas as informações necessárias para espacializar os dado de infiltração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b88277c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lendo Tabelas\n",
      "Lendo Rasteres\n",
      "Len: 83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(5.7120, device='cuda:0', dtype=torch.float64),\n",
       " tensor([9.2089, 2.5374, 1.1951], device='cuda:0', dtype=torch.float64))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Dataset\n",
    "class MeuDataset(Dataset):\n",
    "    nix_bands = [\n",
    "        \"R400 nm\", \"R410 nm\", \"R420 nm\", \"R430 nm\",\n",
    "        \"R440 nm\", \"R450 nm\", \"R460 nm\", \"R470 nm\",\n",
    "        \"R480 nm\", \"R490 nm\", \"R500 nm\", \"R510 nm\",\n",
    "        \"R520 nm\", \"R530 nm\", \"R540 nm\", \"R550 nm\",\n",
    "        \"R560 nm\", \"R570 nm\", \"R580 nm\", \"R590 nm\",\n",
    "        \"R600 nm\", \"R610 nm\", \"R620 nm\", \"R630 nm\",\n",
    "        \"R640 nm\", \"R650 nm\", \"R660 nm\", \"R670 nm\",\n",
    "        \"R680 nm\", \"R690 nm\", \"R700 nm\",\n",
    "    ]\n",
    "\n",
    "    columns_dados = [\"Ponto\", \"Lat\", \"Lon\", \"soils_type\", \"Clay\", \"Silt\", \"Sand\", \"K (C1)\"]\n",
    "\n",
    "# Utilizado nesta ordem para definir o tipo do solo no dataset\n",
    "    soil_types = [\n",
    "        \"Sand\", \"Loamy Sand\", \"Sandy Loam\", \"Loam\",\n",
    "        \"Silt Loam\", \"Silt\", \"Sandy Clay Loam\",\n",
    "        \"Clay Loam\", \"Silty Clay Loam\", \"Sandy Clay\",\n",
    "        \"Silty Clay\", \"Clay\"\n",
    "    ]\n",
    "\n",
    "    def __init__(self, janela:int, device:torch.device|None=None):\n",
    "        \"\"\"\n",
    "        O dataset tem o formato de uma tupla com os valores em X e em Y:\n",
    "        - (X, Y)\n",
    "        - Onde:\n",
    "        - X: (rasters_vals, [Dist. Talvegue, Tipo do Solo, na ordem de soil_types])\n",
    "        - Y: K (cm/s)\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.janela = janela # Janela deve ser ímpar\n",
    "\n",
    "        if self.janela%2 == 0:\n",
    "            raise ValueError(\"A janela deve ser ímpar\")\n",
    "\n",
    "        # Lendo Tabelas\n",
    "        print(\"Lendo Tabelas\")\n",
    "        self.dados = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"Infiltracao\")\n",
    "        self.dados = self.dados[self.columns_dados].dropna().reset_index(drop=True)\n",
    "        gdf = gpd.GeoDataFrame(self.dados, geometry=gpd.points_from_xy(self.dados[\"Lat\"], self.dados[\"Lon\"]), crs=\"EPSG:4326\")\n",
    "        gdf.to_crs(\"EPSG:31983\", inplace=True)\n",
    "        self.dados['Lat'] = gdf.geometry.y\n",
    "        self.dados['Lon'] = gdf.geometry.x\n",
    "\n",
    "        self.nix = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"Nix\")\n",
    "        self.pXRF = pd.read_excel(r\"D:\\Mestrado\\Trabalho Final\\Dados\\Levantamento em Campo\\Compiled.xlsx\", sheet_name=\"pXRF\")\n",
    "\n",
    "        # Lendo Rasteres importantes\n",
    "        self.talvegues = gpd.read_file(r\"D:/Mestrado/Trabalho Final/SIG/HidrografiaArea.zip\")\n",
    "\n",
    "        # Lendo Rasteres\n",
    "        print(\"Lendo Rasteres\")\n",
    "        self.uso_solo        = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/USOSOLO.tif\")                 # Tipos de uso do solo\n",
    "        self.elevation       = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Elevation.tif\")               # Elevação\n",
    "        self.terrain_rug_idx = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/TerrainRuggednessIndex.tif\")  # Variação de elevação entre um pixel e seus vizinhos imediatos\n",
    "        self.topo_pos_idx    = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/TopograficPositionIndex.tif\") # Elevação de um ponto com a média da elevação ao redor, topo, vale ou plano\n",
    "        self.roughness       = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Roughness.tif\")               # A diferença entre a elevação máxima e mínima dentro de uma vizinhança\n",
    "        self.slope           = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Slope.tif\")                   # Declividade\n",
    "        self.aspect          = rxr.open_rasterio(r\"D:/Mestrado/Trabalho Final/SIG/Aspect.tif\")                  # Para onde \"aponta\" a face do terreno\n",
    "\n",
    "        # Dados dos rasteres concatenados\n",
    "        self.raster_data = np.array([\n",
    "            self.uso_solo.values[0],\n",
    "            self.elevation.values[0],\n",
    "            self.terrain_rug_idx.values[0],\n",
    "            self.topo_pos_idx.values[0],\n",
    "            self.roughness.values[0],\n",
    "            self.slope.values[0],\n",
    "            self.aspect.values[0],\n",
    "        ])\n",
    "\n",
    "        self.transformer = Transformer.from_crs(\"EPSG:31983\", self.uso_solo.rio.crs, always_xy=True)\n",
    "        self.transform = self.uso_solo.rio.transform()\n",
    "\n",
    "        self._process_dados()\n",
    "\n",
    "    def _process_dados(self):\n",
    "        # Dados para convolução\n",
    "        x, y = self.transformer.transform(self.dados[\"Lon\"], self.dados[\"Lat\"])\n",
    "        row, col = rowcol(self.transform, x, y)\n",
    "\n",
    "        jan = int((self.janela-1)/2)\n",
    "\n",
    "        start_row = row-jan\n",
    "        end_row   = row+jan\n",
    "\n",
    "        start_col = col-jan\n",
    "        end_col   = col+jan\n",
    "\n",
    "        self.dados['s_row']=start_row\n",
    "        self.dados['e_row']=end_row\n",
    "        self.dados['s_col']=start_col\n",
    "        self.dados['e_col']=end_col\n",
    "\n",
    "        # Distância até o talvegue principal\n",
    "        linhas_unidas = self.talvegues.union_all()\n",
    "        pontos = gpd.GeoSeries(gpd.points_from_xy(self.dados[\"Lon\"], self.dados[\"Lat\"]), crs=\"EPSG:31983\")\n",
    "        dists = pontos.apply(lambda p: p.distance(linhas_unidas))\n",
    "        self.dados[\"dist_talvegue\"] = dists\n",
    "\n",
    "        for soil_type in self.soil_types:\n",
    "            self.dados[soil_type] = np.where(self.dados[\"soils_type\"]==soil_type, 1, 0)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dados)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = i\n",
    "        if isinstance(i, (int, float)):\n",
    "            idx = [i]\n",
    "\n",
    "        # Pontos\n",
    "        pontos = self.dados.loc[idx]\n",
    "\n",
    "        # Valores nos rasteres\n",
    "        rasters_vals = []\n",
    "        for s_row, e_row, s_col, e_col in zip(pontos['s_row'], pontos['e_row'], pontos['s_col'], pontos['e_col']):\n",
    "            rasters_vals.append(self.raster_data[:, s_row:e_row+1, s_col:e_col+1])\n",
    "        \n",
    "        rasters_vals = torch.tensor(np.array(rasters_vals), device=self.device, dtype=torch.float64)\n",
    "\n",
    "        # Demais dados\n",
    "        K = torch.tensor(pontos[\"K (C1)\"].values, device=self.device) * 1000                                # K*1000 pois os valores estão baixos de mais\n",
    "        dist_talvegue = torch.tensor(pontos[[\"dist_talvegue\"]+self.soil_types].values, device=self.device)\n",
    "\n",
    "        if isinstance(i, (int, float)):\n",
    "            dist_talvegue = dist_talvegue[0]\n",
    "            rasters_vals = rasters_vals[0]\n",
    "            K = K[0]\n",
    "\n",
    "\n",
    "        return (rasters_vals, dist_talvegue), K\n",
    "    \n",
    "dataset = MeuDataset(janela = 25, device=device)\n",
    "\n",
    "print(\"Len:\", len(dataset))\n",
    "dataset[15][1], dataset[1:3][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d1aafd",
   "metadata": {},
   "source": [
    "### Configurações do treino\n",
    "---\n",
    "\n",
    "- Seed para números aleatórios\n",
    "- % de treino e teste\n",
    "- Métricas\n",
    "- Epochs\n",
    "- Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed para permitir reprodutibilidade dos valores pseudo-aleatórios\n",
    "seed = 42\n",
    "\n",
    "# Porcentagens de Treino e Teste\n",
    "train_percent = 85\n",
    "test_percent  = 15\n",
    "\n",
    "# BatchSize e Epochs\n",
    "batch_size = 2\n",
    "epochs     = 1000  # Poucos pontos, verificar overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9baf77e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N Total: 83 N Train: 70 N Teste: 13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([4.5921, 4.9097, 8.8189, 9.0623], device='cuda:0', dtype=torch.float64),\n",
       " tensor([13.8117,  2.4143,  0.8428,  8.4564], device='cuda:0',\n",
       "        dtype=torch.float64))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = len(dataset)\n",
    "n_train = int(train_percent*n/100)\n",
    "n_test = n - n_train\n",
    "g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "train_ds, test_ds = random_split(dataset, [n_train, n_test], generator=g)\n",
    "\n",
    "print(\"N Total:\", n, \"N Train:\", n_train, \"N Teste:\", n_test)\n",
    "\n",
    "train_ds[1:5][1], test_ds[1:5][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd0f5b7",
   "metadata": {},
   "source": [
    "### MLP e CNN\n",
    "---\n",
    "\n",
    "MLP configurada com uma CNN também"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b64aa68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4290],\n",
       "        [0.4293],\n",
       "        [0.4317],\n",
       "        [0.4315]], device='cuda:0', dtype=torch.float64,\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo (MLP e CNN)\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=7,    # Número de bandas do raster de entrada\n",
    "                out_channels=16,\n",
    "                kernel_size=3,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        ).to(dtype=torch.float64)\n",
    "        self.flatten = nn.Flatten().to(dtype=torch.float64)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(\n",
    "                (1152) + 12 + 1, # Tamanho da saída da convolução + 12 tipos de solo + dist_talvegue\n",
    "                64\n",
    "            ),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16, 1)\n",
    "        ).to(dtype=torch.float64)\n",
    "\n",
    "    def forward(self, raster, values):\n",
    "        x = self.conv(raster)\n",
    "        x = self.flatten(x)\n",
    "        x = torch.cat([x, values], dim=1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "mlp = MLP().to(device=device)\n",
    "\n",
    "X = dataset[1:4][0]\n",
    "mlp(*X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f9fcac",
   "metadata": {},
   "source": [
    "### Ajustar o modelo MLP\n",
    "\n",
    "- Processos para ajustar a MLP pelo método do gradiente descendente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f431267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/500 | Train Loss: 95.21851814 | Val Loss: 154.524369428\r"
     ]
    }
   ],
   "source": [
    "lr = 0.01 # Learning Rate\n",
    "\n",
    "os.makedirs(\"best_model/\", exist_ok=True)\n",
    "\n",
    "model = MLP().to(device=device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", patience=3, factor=0.5) # Learning Rate Variável\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=batch_size)\n",
    "\n",
    "best_test_loss = torch.inf\n",
    "best_state = None\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    # --- treino ---\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for x, y in train_loader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        y_sim = model(*x)\n",
    "\n",
    "        loss = criterion(y_sim[:, 0], y)\n",
    "        running_loss += loss.item() * x[1].size(0)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step() # Altero a taxa de aprendizado a cada passo\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader.dataset) # type: ignore\n",
    "\n",
    "    # --- validação ---\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            # Forward\n",
    "            y_sim = model(*x)\n",
    "\n",
    "            loss = criterion(y_sim[:, 0], y)\n",
    "            val_loss += loss.item() * x[1].size(0)\n",
    "\n",
    "    # Média do loss\n",
    "    val_loss /= len(test_loader.dataset) # type: ignore\n",
    "\n",
    "    os.system(\"cls\")\n",
    "    print(f\"Epoch {epoch}/{epochs} | Train Loss: {train_loss:.8f} | Val Loss: {val_loss:.8f}\", end=\"\\r\", flush=True)\n",
    "\n",
    "    # early stopping simples\n",
    "    if val_loss < best_test_loss:\n",
    "        best_test_loss = val_loss\n",
    "        best_state = {\n",
    "            \"lr\":lr,\n",
    "            \"epoch\": epoch,\n",
    "            \"epochs\":epochs,\n",
    "            \"val_loss\":val_loss,\n",
    "            \"train_loss\":train_loss,\n",
    "            \"batch_size\":batch_size,\n",
    "            \"model_state\":model.state_dict(),\n",
    "        }\n",
    "\n",
    "        torch.save(best_state, f\"best_model/{str(val_loss).replace(\".\", \"_\")}.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "libs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
