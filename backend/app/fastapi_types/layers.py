from typing import Union, Tuple, Optional, Any
from pydantic import BaseModel, Field
import torch.nn as nn

class Linear(BaseModel):
    desc: str = """
## Camada Linear (`nn.Linear`)

### O que Ã©?
A camada linear (tambÃ©m chamada de **fully connected** ou **densa**) Ã© a pedra fundamental das redes neurais. Ela realiza uma transformaÃ§Ã£o matemÃ¡tica onde:
- `x`: Dados de entrada  
- `W`: Matriz de pesos (aprendida durante o treinamento)  
- `b`: Termo de bias (opcional)  
- `y`: SaÃ­da transformada

### ParÃ¢metros de ConfiguraÃ§Ã£o

| ParÃ¢metro       | Tipo | DescriÃ§Ã£o                                                                 | Exemplo Valido |
|-----------------|------|---------------------------------------------------------------------------|----------------|
| `in_features`   | int  | NÃºmero de caracterÃ­sticas de entrada                                      | `256`          |
| `out_features`  | int  | NÃºmero de neurÃ´nios na camada (dimensÃ£o da saÃ­da)                         | `128`          |
| `bias`          | bool | Se `True`, adiciona um termo de ajuste (bias) a cada neurÃ´nio (recomendado) | `True`         |

### Como Funciona na PrÃ¡tica
1. **Num sistema de recomendaÃ§Ã£o**:  
   - Entrada: CaracterÃ­sticas do usuÃ¡rio (idade, histÃ³rico)  
   - SaÃ­da: Probabilidade de gostar de um produto  

2. **Em diagnÃ³stico mÃ©dico**:  
   - Entrada: Sintomas do paciente  
   - SaÃ­da: Chance de ter uma doenÃ§a especÃ­fica  

### Comportamento Durante o Treinamento
- A matriz `W` Ã© ajustada automaticamente para aprender padrÃµes nos dados
- O bias (`b`) ajuda a deslocar a funÃ§Ã£o de ativaÃ§Ã£o quando necessÃ¡rio
- Cada neurÃ´nio na camada opera independentemente nos dados

### Exemplo de CÃ³digo
```python
# Cria uma camada que recebe 256 valores e produz 128 saÃ­das
layer = nn.Linear(in_features=256, out_features=128)
input_data = torch.randn(32, 256)  # Batch de 32 amostras
output = layer(input_data)  # Formato: (32, 128)
    """
    in_features: int
    out_features: int
    bias: bool = True

    def __call__(self) -> nn.Module:
        return nn.Linear(
            in_features=self.in_features,
            out_features=self.out_features,
            bias=self.bias
        )

class Conv1d(BaseModel):
    desc: str = """
## Camada Conv1D (`nn.Conv1d`)

### ðŸ” O que Ã©?
A Conv1D Ã© a camada de **convoluÃ§Ã£o unidimensional** do PyTorch, especializada em processar:
- **SÃ©ries temporais** (dados financeiros, sensores)
- **Sinais de Ã¡udio** (onda sonora)
- **SequÃªncias textuais** (quando representadas como embeddings)

### ðŸ“Š ParÃ¢metros Principais
| ParÃ¢metro         | Tipo        | DescriÃ§Ã£o                                                                 | Exemplo VÃ¡lido   |
|-------------------|-------------|---------------------------------------------------------------------------|------------------|
| `in_channels`     | `int`       | NÃºmero de canais de entrada                                               | `1` (audio mono) |
| `out_channels`    | `int`       | NÃºmero de filtros/kernels                                                 | `64`             |
| `kernel_size`     | `int`       | Tamanho da janela deslizante (em amostras)                                | `3` ou `5`       |
| `stride`         | `int`       | Passo do filtro (default: 1)                                              | `2`              |
| `padding`        | `int`       | Zeros adicionados nas bordas (para controlar tamanho da saÃ­da)            | `1`              |
| `dilation`       | `int`       | EspaÃ§amento entre elementos do kernel (para capturar padrÃµes distantes)    | `2`              |
| `bias`           | `bool`      | Se adiciona termo de bias (default: `True`)                               | `False`          |

### ðŸ› ï¸ Como Funciona?
Cada filtro:
1. Desliza ao longo da sequÃªncia
2. Realiza multiplicaÃ§Ãµes pontuais
3. Gera um **mapa de caracterÃ­sticas** unidimensional
"""
    in_channels: int
    out_channels: int
    kernel_size:int
    stride:int = 1
    padding:int = 0
    
    def __call__(self) -> nn.Module:
        return nn.Conv1d(
            in_channels=self.in_channels,
            out_channels=self.out_channels,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding
        )

class Conv2d(BaseModel):
    desc: str = """
## Camada de ConvoluÃ§Ã£o 2D (`nn.Conv2d`)

### O que Ã©?
A camada Conv2D Ã© o "olho" das redes neurais para processamento de imagens. Ela desliza pequenos filtros (kernels) sobre a imagem para detectar padrÃµes visuais, como bordas, texturas ou objetos. Funciona como um scanner inteligente que:

- Analisa pequenas regiÃµes da imagem de cada vez
- Aprende padrÃµes hierÃ¡rquicos (de simples bordas atÃ© objetos complexos)
- Preserva relaÃ§Ãµes espaciais entre pixels

### ParÃ¢metros de ConfiguraÃ§Ã£o

| ParÃ¢metro        | Tipo               | DescriÃ§Ã£o                                                                 | Exemplo VÃ¡lido       |
|------------------|--------------------|---------------------------------------------------------------------------|----------------------|
| `in_channels`    | int                | NÃºmero de canais de entrada (1 para preto e branco, 3 para RGB)           | `3`                  |
| `out_channels`   | int                | NÃºmero de filtros/kernels (cada um aprende um padrÃ£o diferente)           | `64`                 |
| `kernel_size`    | int ou (int, int)  | Tamanho da janela do filtro (altura x largura)                            | `3` ou `(5,3)`       |
| `stride`         | int ou (int, int)  | Passo do filtro (controla como ele se move na imagem)                     | `1` ou `(2,2)`       |
| `padding`        | int ou (int, int)  | Zeros adicionados nas bordas para controlar o tamanho da saÃ­da            | `0` ou `(1,1)`       |
| `bias`           | bool               | Se adiciona um termo de ajuste a cada filtro (normalmente True)           | `True`               |

### Como Funciona na PrÃ¡tica
1. **Reconhecimento Facial**:
   - Filtros iniciais detectam bordas
   - Camadas profundas reconhecem olhos, nariz, etc

2. **DiagnÃ³stico por Imagem**:
   - Primeiras camadas identificam texturas
   - Camadas posteriores detectam anomalias como tumores

### Comportamento Durante o Treinamento
- Cada filtro/kernel aprende a responder a um padrÃ£o visual especÃ­fico
- Os parÃ¢metros sÃ£o ajustados para maximizar a detecÃ§Ã£o de caracterÃ­sticas Ãºteis
- OperaÃ§Ã£o local: cada filtro sÃ³ "enxerga" uma pequena regiÃ£o por vez

### Exemplo de CÃ³digo
```python
# Cria uma camada para processar imagens RGB (3 canais) com 64 filtros 3x3
conv_layer = nn.Conv2d(
    in_channels=3,
    out_channels=64,
    kernel_size=3,
    stride=1,
    padding=1
)

# Processa um lote de 16 imagens 128x128 RGB
input_images = torch.randn(16, 3, 128, 128)  # Formato: (batch, canais, altura, largura)
output = conv_layer(input_images)  # SaÃ­da: (16, 64, 128, 128)
    """
    in_channels: int
    out_channels: int
    kernel_size: Union[int, Tuple[int, int]]
    stride: Union[int, Tuple[int, int]] = 1
    padding: Union[int, Tuple[int, int]] = 0
    bias: bool = True

    def __call__(self) -> nn.Module:
        return nn.Conv2d(
            in_channels=self.in_channels,
            out_channels=self.out_channels,
            kernel_size=self.kernel_size,
            stride=self.stride,
            padding=self.padding,
            bias=self.bias
        )

class LSTM(BaseModel):
    desc: str = """
## Camada LSTM (`nn.LSTM`)

### O que Ã©?
A LSTM (Long Short-Term Memory) Ã© uma rede neural especializada em processar **sequÃªncias de dados** (como texto, sÃ©ries temporais ou Ã¡udio), com capacidade de memorizar informaÃ§Ãµes importantes por longos perÃ­odos.

### ParÃ¢metros Principais

| ParÃ¢metro       | O que significa?                                                                 | Exemplo |
|-----------------|---------------------------------------------------------------------------------|---------|
| `input_size`    | NÃºmero de caracterÃ­sticas em cada passo da sequÃªncia (ex: 128 para embeddings)   | `128`   |
| `hidden_size`   | Tamanho da memÃ³ria oculta (quantos neurÃ´nios a LSTM terÃ¡ internamente)          | `256`   |
| `num_layers`    | Quantas LSTMs empilhadas (1 = bÃ¡sica, >1 = mais complexa)                       | `2`     |

### SaÃ­das
A LSTM retorna dois tipos de informaÃ§Ã£o:

1. **`output`**  
   - ContÃ©m **todas as previsÃµes** em cada passo da sequÃªncia  
   - Formato: `(tamanho_da_sequÃªncia, batch_size, hidden_size)`  
   - *Exemplo:* Se processar 10 palavras, retorna 10 resultados intermediÃ¡rios

2. **`(h_n, c_n)`**  
   - `h_n`: Ãšltima saÃ­da da rede (o "resumo" de toda a sequÃªncia)  
   - `c_n`: Estado interno final (a "memÃ³ria" acumulada)  
   - Formato: `(num_layers, batch_size, hidden_size)`  
   - *Uso comum:* `h_n` Ã© Ã³timo para classificaÃ§Ã£o final

### Exemplo PrÃ¡tico
Imagine que estÃ¡ processando uma frase:
```python
lstm = nn.LSTM(input_size=128, hidden_size=256, num_layers=2)
# Frase com 5 palavras, cada uma representada por 128 nÃºmeros
input = torch.randn(5, 1, 128)  # (sequÃªncia, batch, caracterÃ­sticas)

output, (h_n, c_n) = lstm(input)

## Entendendo `num_layers` na LSTM

### O que Ã©?
O parÃ¢metro `num_layers` define quantas **camadas de LSTM estÃ£o empilhadas** uma sobre a outra. Funciona como uma escada de processamento:

- `num_layers=1` (PadrÃ£o):  
  â†’ Uma Ãºnica LSTM processa a sequÃªncia  
  â†’ Equivalente a um andar Ãºnico de processamento

- `num_layers>1` (LSTM Profunda):  
  â†’ A saÃ­da da primeira LSTM vira entrada da prÃ³xima  
  â†’ Cada camada aprende padrÃµes mais complexos  
  â†’ Como uma equipe de especialistas onde cada um refina o trabalho do anterior

### Exemplo Visual
# Arquitetura com 3 camadas:
input â†’ LSTM_Camada1 â†’ LSTM_Camada2 â†’ LSTM_Camada3 â†’ output
    """
    input_size: int
    hidden_size: int
    num_layers: int = 1

    def __call__(self) -> nn.Module:
        return nn.LSTM(
            input_size=self.input_size,
            hidden_size=self.hidden_size,
            num_layers=self.num_layers
        )

class Dropout(BaseModel):
    desc: str = """
    TÃ©cnica de regularizaÃ§Ã£o que "desliga" aleatoriamente neurÃ´nios durante o treinamento.
    Previne que a rede neural dependa demais de poucos neurÃ´nios especÃ­ficos.
    
    Exemplo prÃ¡tico:
    - Funciona como um time de futebol que treina rodiziando jogadores, evitando depender sÃ³ do astro.
    - Num sistema de crÃ©dito, forÃ§a o modelo a considerar mÃºltiplos fatores de risco, nÃ£o sÃ³ o score.
    """
    p: float = Field(0.5, ge=0, le=1)

    def __call__(self) -> nn.Module:
        return nn.Dropout(p=self.p)

class BatchNorm2d(BaseModel):
    desc: str = """
    Normaliza os dados entre camadas para manter a escala consistente durante o treinamento.
    Equivale a colocar todos os recursos na mesma "rÃ©gua" antes de processar.
    
    Exemplo prÃ¡tico:
    - Num sistema que analisa exames de sangue, padroniza diferentes escalas (glicose em mg/dL, colesterol em mmol/L).
    - Em processamento de imagens, ajusta automaticamente brilho e contraste entre diferentes fotos.
    """
    num_features: int

    def __call__(self) -> nn.Module:
        return nn.BatchNorm2d(num_features=self.num_features)

class MultiheadAttention(BaseModel):
    desc: str = """
    Mecanismo de atenÃ§Ã£o que permite ao modelo focar em partes diferentes da entrada simultaneamente.
    Como um time de especialistas onde cada um analisa um aspecto diferente dos dados.
    
    Exemplo prÃ¡tico:
    - Em traduÃ§Ã£o automÃ¡tica, identifica sujeito, verbo e objeto em frases longas.
    - Em anÃ¡lise de contratos, destaca clÃ¡usulas importantes em diferentes seÃ§Ãµes do documento.
    """
    embed_dim: int
    num_heads: int

    def __call__(self) -> nn.Module:
        return nn.MultiheadAttention(
            embed_dim=self.embed_dim,
            num_heads=self.num_heads
        )

class ReLU(BaseModel):
    desc: str = """
# ReLU (Rectified Linear Unit)
**FÃ³rmula**: `max(0, x)`

- Elimina gradientes negativos (vanishing gradient para x < 0)
- Computacionalmente eficiente
- Pode causar "neurÃ´nios mortos" (dead ReLU)
"""
    inplace: bool = False

    def __call__(self) -> nn.Module:
        return nn.ReLU(inplace=self.inplace)

class Sigmoid(BaseModel):
    desc: str = """
# Sigmoid
**FÃ³rmula**: `1 / (1 + exp(-x))`

- SaÃ­da entre 0 e 1 (Ãºtil para probabilidades)
- Problemas com vanishing gradient em extremos
- Usada em classificaÃ§Ã£o binÃ¡ria
"""

    def __call__(self) -> nn.Module:
        return nn.Sigmoid()

class Tanh(BaseModel):
    desc: str = """
# Tanh (Tangente HiperbÃ³lica)
**FÃ³rmula**: `(exp(x) - exp(-x)) / (exp(x) + exp(-x))`

- SaÃ­da entre -1 e 1
- MÃ©dia zero (melhor que sigmoid para alguns casos)
- Still suffers from vanishing gradients
"""

    def __call__(self) -> nn.Module:
        return nn.Tanh()

class LeakyReLU(BaseModel):
    desc: str = """
# LeakyReLU
**FÃ³rmula**: `max(Î±x, x)` onde Î± Ã© pequeno (ex: 0.01)

- Tenta resolver o problema dos "neurÃ´nios mortos" do ReLU
- Permite um pequeno gradiente para valores negativos
"""
    negative_slope: float = 0.01
    inplace: bool = False

    def __call__(self) -> nn.Module:
        return nn.LeakyReLU(
            negative_slope=self.negative_slope,
            inplace=self.inplace
        )

class GELU(BaseModel):
    desc: str = """
# GELU (Gaussian Error Linear Unit)
**FÃ³rmula**: `x * Î¦(x)` (onde Î¦ Ã© a CDF da distribuiÃ§Ã£o normal)

- AproximaÃ§Ã£o suave do ReLU
- Usada em modelos como GPT e BERT
- Computacionalmente mais cara
"""

    def __call__(self) -> nn.Module:
        return nn.GELU()

class SiLU(BaseModel):
    desc: str = """
# SiLU (Sigmoid-Weighted Linear Unit)
**FÃ³rmula**: `x * sigmoid(x)`

- TambÃ©m chamada de Swish (paper do Google)
- Combina benefÃ­cios do ReLU e sigmoid
- Usada em arquiteturas modernas
"""

    def __call__(self) -> nn.Module:
        return nn.SiLU()

class Softmax(BaseModel):
    desc: str = """
# Softmax
**FÃ³rmula**: `exp(x_i) / Î£(exp(x_j))`

- SaÃ­da Ã© distribuiÃ§Ã£o de probabilidade (soma = 1)
- Usada em classificaÃ§Ã£o multiclasse
- SensÃ­vel a grandes diferenÃ§as nos valores de entrada
"""
    dim: Optional[int] = None

    def __call__(self) -> nn.Module:
        return nn.Softmax(dim=self.dim)

class MoE(BaseModel):
    desc: str = """
## ðŸ§  Mixture of Experts (MoE) - O que Ã©?

Sim, a **Mixture of Experts** (Mistura de Especialistas) Ã© uma arquitetura avanÃ§ada de redes neurais que:

1. **Divide o problema** em sub-tarefas especializadas  
2. **Roteia dinamicamente** cada entrada para os "especialistas" mais relevantes  
3. **Combina inteligentemente** os resultados parciais

### ðŸ”§ Como Funciona?
| Componente       | FunÃ§Ã£o                                                                 |
|------------------|-----------------------------------------------------------------------|
| **Experts**      | Pequenas redes especializadas (ex: uma para texto, outra para imagens) |
| **Gating Network** | Rede de "controle" que decide quais experts usar para cada entrada    |
| **Routing**      | Sistema dinÃ¢mico que encaminha dados (sparse/dinÃ¢mico)                |

### ðŸ“Š Exemplo TÃ©cnico (PyTorch-like)
```python
class MixtureOfExperts(nn.Module):
    def __init__(self, num_experts, hidden_size):
        super().__init__()
        self.experts = nn.ModuleList([ExpertNetwork() for _ in range(num_experts)])
        self.gate = nn.Linear(hidden_size, num_experts)  # Rede de decisÃ£o

    def forward(self, x):
        # 1. O gate decide quais experts usar
        weights = F.softmax(self.gate(x), dim=-1)  # Probabilidades
        
        # 2. Roteamento esparso (top-k experts)
        top_k_weights, top_k_experts = torch.topk(weights, k=2)
        
        # 3. Processamento distribuÃ­do
        outputs = 0
        for i, expert in enumerate(self.experts):
            mask = (top_k_experts == i)
            if mask.any():
                outputs += expert(x) * top_k_weights[mask]
        
        return outputs
"""
    experts: list[Union[LSTM, GELU, SiLU, ReLU, Tanh, Linear, Conv1d, Conv2d, Softmax, Dropout, Sigmoid, LeakyReLU, BatchNorm2d, MultiheadAttention]]
    out_features: int
    bias: bool = True

LayerType = Union[MoE, LSTM, GELU, SiLU, ReLU, Tanh, Linear, Conv1d, Conv2d, Softmax, Dropout, Sigmoid, LeakyReLU, BatchNorm2d, MultiheadAttention]